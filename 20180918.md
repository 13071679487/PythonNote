# 20180918

### 作业

```xml
	用程序实现：http://www.doutula.com/photo/list/?page=4
	该网站指定页的爬取，并将指定gif图片保存到文件中。
```



### 代码仅做参考

```python
import re
import requests
import os
from multiprocessing import Pool


HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36'}
TARGET_DIR = r'D:\\Python\mp4'
CURRENT_DIR = os.getcwd()

def checkStatus_code(status_code):
    if not status_code:
        print("Fail to connect the url")
        return False
    if status_code == 200:
        return True
    elif status_code == 301:
        print("The target url has redirect")
    elif status_code == 404:
        print("The target is not exist")
    elif status_code == 403:
        print("You has not authorization")
    elif status_code == 502:
        print("The server has meeting error")
    else:
        print(f"You has met a unknown error\t%d" % status_code)
    return False

def parseHref(url):
    if not url:
        print('unknown')
        return
    resp = requests.get(url, headers=HEADERS)
    if checkStatus_code(resp.status_code):
        pattern = r'class="img-responsive lazy image_dta".*?data-original="(.*?)"'
        # print(resp.text)
        imageUrls = re.findall(pattern, resp.text, re.S)
        return imageUrls

def downFromUrl(url):
    index = url.rfind('/')
    fileName = url[index + 1:]
    resp = requests.get(url,headers=HEADERS)
    if checkStatus_code(resp.status_code):
        with open(os.path.join(TARGET_DIR, fileName), 'wb') as f:
            f.write(resp.content)

def prepare2Down(urls):
    pool = Pool()
    for url in urls:
        pool.apply_async(downFromUrl, args=(url,))
    pool.close()
    pool.join()




if __name__ == '__main__':
    if len(TARGET_DIR) == 0:
        TARGET_DIR = CURRENT_DIR
    imageUrls = parseHref('http://www.doutula.com/photo/list/?page=4')
    prepare2Down(imageUrls)
```





### 笔记部分

~~~python
网络爬虫
1、概念：
	   模拟浏览器发送一样的网络请求，自动地抓取网页信息的程序或脚本。
	通俗的说：使用事先写好的程序去抓取网络上所需要的数据。

2、分类：
	1、通用爬虫：
		概念：爬行对象从一些种子 URL 扩充到整个 Web。
		应用：主要为门户站点搜索引擎和大型 Web 服务提供商采集数据。
		缺点：网络爬虫的爬行范围和数量巨大，对于爬行速度和存储空间要求较高，对于爬行页面的顺序要求相对较低，同时由于待刷新的页面太多，通常采用并行工作方式
		结构：页面爬行模块 、页面分析模块、链接过滤模块、页面数据库、URL 队列、初始 URL 集合几个部分。
		策略：深度优先策略、广度优先策略。
			1、深度优先策略：其基本方法是按照深度由低到高的顺序，依次访问下一级网页链接，直到不能再深入为止。
			2、广度优先策略：此策略按照网页内容目录层次深浅来爬行页面，处于较浅目录层次的页面首先被爬行。 当同一层次中的页面爬行完毕后，爬虫再深入下一层继续爬行。
	2、聚焦爬虫：
		概念：针对某一特定领域的数据进行抓取的程序。
		优点：极大地节省了硬件和网络资源，保存的页面也由于数量少而更新快。

3、爬虫各等级所需知识：
	1、Web前端的知识：HTML, CSS, JavaScript, DOM, DHTML, Ajax, jQuery,json等
	2、正则表达式（re）：提取正常一般网页中想要的信息，比如某些特殊的文字，链接信息，知道贪婪模式与懒惰模式。
	3、使用re, BeautifulSoup，XPath等获取一些DOM结构中的节点信息.
	4、使用urllib，urllib2或requests库进行简单的数据抓取。
	5、熟悉HTTP,HTTPS协议的基础知识,了解GET，POST方法,了解HTTP头中的信息，包括返回状态码，编码，user-agent，cookie，session等。（初级）
	6、设置User-Agent进行数据爬取，设置代理等。
	7、使用Fiddle, Wireshark等工具抓取及分析简单的网络数据包。
	8、对于动态爬虫要学会分析Ajax请求，模拟制造Post数据包请求，抓取客户端session等信息，对于一些简单的网站，能够通过模拟数据包进行自动登录
	9、会使用phatomjs+selenium抓取一些动态网页信息。
	10、并发下载，通过并行下载加速数据抓取，多线程的使用。
	11、能使用Tesseract，百度AI等库进行验证码识别。（中级）
	12、会使用常用的数据库（Mongodb，Redis）进行数据存储，查询
	13、能使用机器学习的技术动态调整爬虫的爬取策略，从而避免被禁IP封号等。
	14、使用一些开源框架Scrapy，Scray+Redis，Celery等分布式爬虫，能部署掌控分布式爬虫进行大规模的数据抓取。（高级）

4、爬虫的设计思路：
	1、确定爬取网页的URL，如果要爬取下一个URl地址，则需要构建
	2、通过HTTP/HTTPS协议来获取对应的HTML语言（发送请求、获取响应）
	3、提取HTML页面中有用的数据。
	4、保存提取的数据。

5、抓取HTML页面：
	网页的三大特征：
		1、每个网页都有它唯一的URL
		2、每个网页都是使用HTML来描述页面的信息。
		3、网页都是通过HTTP/HTTPS协议来传输HTML数据。

```
HTTP请求处理的库：urllib、urllib2、requests（urllib2应用于Python2版本）
HTTP响应内容的处理：re、xpath、beautifulSoup(bs4)
```

6、基本常识：
	1、Robots协议：
		Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”：网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。
		淘宝网：https://www.taobao.com/robots.txt
		腾讯网：http://www.qq.com/robots.txt

```
2、估算网站的大小:
	site:DNS
	只是通过百度搜索引擎大致来估算网站的大小，受到网站本身对搜索引擎爬虫的限制，及搜索引擎本身爬取数据技术的限制，这只是一个估计值。

3、识别网站所用技术：
	import builtwith
	builtwith.parse(‘http://www.baidu.com’)
    # {'javascript-frameworks': ['jQuery']}
```

7、HTTP和HTTPS
	HTTP协议（HyperText Transfer Protocol，超文本传输协议）：是一种发布和接收 HTML页面的方法。

```
HTTPS（Hypertext Transfer Protocol over Secure Socket Layer）简单讲是HTTP的安全版，在HTTP下加入SSL层。

SSL（Secure Sockets Layer 安全套接层）主要用于Web的安全传输协议，在传输层对网络连接进行加密，保障在Internet上数据传输的安全。
•HTTP的端口号为80，
•HTTPS的端口号为443

HTTP通信由两部分组成： 客户端请求消息 与 服务器响应消息

浏览器发送HTTP请求的过程：
	1.当用户在浏览器的地址栏中输入一个URL并按回车键之后，浏览器会向HTTP服务器发送HTTP请求。HTTP请求主要分为“Get”和“Post”两种方法。

	2.浏览器发送一个Request请求去获取 http://www.baidu.com 的html文件，服务器把Response文件对象发送回给浏览器。

	3.浏览器分析Response中的 HTML，再次发送Request去获取图片，CSS文件，或者JS文件。

	4.当所有的文件都下载成功后，网页会根据HTML语法结构，完整的显示出来。



8、URL
	（Uniform / Universal Resource Locator的缩写）：统一资源定位符，是用于完整地描述Internet上网页和其他资源的地址的一种标识方法。

基本格式：scheme://host[:port#]/path/…/[?query-string][#anchor]
•scheme：协议(例如：http, https, ftp)
•host：服务器的IP地址或者域名
•port#：服务器的端口（如果是走协议默认端口，缺省端口80）
•path：访问资源的路径
•query-string：参数，发送给http服务器的数据
•anchor：锚（跳转到网页的指定锚点位置）


9.get与post请求：
	get 请求指定的页面信息，并返回实体主体。从服务器上获取数据，显示在浏览器网址上

POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件），数据被包含在请求体中。消息长度没有限制而且以隐式的方式进行发送，通常用来向HTTP服务器提交量比较大的数据
```

10、5、User-Agent:
		用户代理，它是一个特殊字符串，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等
		header = {‘User-Agent’: ‘Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36’}

11、requests模块：
	安装：python –m pip install requests

import requests

reponse = requests.get(url, headers, timeout)  # 发送get请求		        
reponse = requests.post(url, data={请求的字典)	 # 发送post请求

req = requests.get(url, headers=header)
print(req.status_code)  # 打印响应码
print(req.encoding)  # 打印网页的编码格式
print(req.headers)  # 打印网页的响应头信息
print(req.content)  # 打印响应内容
```

11、获取页面内容与编码：
	1、reponse.encoding = ‘utf-8’	 # 设置响应内容的格式
		print(reponse.text)      	 # 获取响应的网页内容
	2、reponse.content.decode()	 # 获取响应的内容（byte【二进制字节】）

	3、reponse.content.decode(‘utf-8’)	# 获取的内容以‘utf-8’格式解码
~~~



