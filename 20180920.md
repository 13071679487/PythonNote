# 20180920





### 代码仅做参考

```python
from lxml import etree
import requests
import os
import re
from bs4 import BeautifulSoup as BS
from threading import Thread

#该程序是爬趣书阁的科幻文章
#

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36'}
COOKIE = ''
cookies = {items.split('=')[0]:items.split('=')[1] for items in COOKIE.split('; ')}

DATA={}
TARGET_DIR = r'D:\\Python\PythonProjects\Python-01\resp'
CURRENT_DIR = os.getcwd()

def checkStatus_code(status_code):
    if not status_code:
        print("Fail to connect the url")
        return False
    if status_code == 200:
        return True
    elif status_code == 301:
        print("The target url has redirect")
    elif status_code == 404:
        print("The target is not exist")
    elif status_code == 403:
        print("You has not authorization")
    elif status_code == 502:
        print("The server has meeting error")
    else:
        print(f"You has met a unknown error\t%d" % status_code)
    return False

def downFromUrl(url):
    if url:
        try:
            resp = requests.get(url, headers=HEADERS)
        except:
            print(f'{url}\t连接失败，跳过')
            pass
        else:
            if checkStatus_code(resp.status_code):
                index = url.rfind('/')
                fileName = url[index + 1:]
                try:
                    with open(os.path.join(TARGET_DIR, fileName), 'wb') as f:
                        f.write(resp.content)
                except:
                    print(f'下载失败-->{url}')
                    pass

def saveResp2HTML(resp,fileName):
    try:
        with open(os.path.join(TARGET_DIR, fileName), 'w', encoding='utf-8') as f:
            f.write(resp.content.decode('utf-8'))
    except:
        print(f'{fileName}\t写入文件失败')
        pass

#获取到小说列表
def parseArticleUrl(jianshuUrl):
    if jianshuUrl:
        try:
            resp = requests.get(jianshuUrl, headers=HEADERS)
        except:
            print(f'{jianshuUrl}\t连接失败，跳过')
        else:
            if checkStatus_code(resp.status_code):
                fictions = []
                Thread(target=saveResp2HTML, args=(resp, 'resp-fiction.html',)).start()
                soup = BS(resp.text, 'lxml')
                ul = soup.select('.up li')
                for li in ul:
                    try:
                        type = li.select('span[class="s1"]')[0].get_text()
                        title = li.select('span[class="s2"]')[0].a.get_text()
                        fictionUrl =li.select('span[class="s2"]')[0].a['href']
                        author = li.select('span[class="s4"]')[0].get_text()
                        updateTime = li.select('span[class="s5"]')[0].get_text()
                        print(f'{title}\t\t{fictionUrl}\t\t{author}')
                        fictions.append([title, fictionUrl, type, author, updateTime])
                    except:
                        pass
                return fictions
    return None

#获取到小说的章节列表
def getFictionDetailFromUrl(fictionUrl,title):
    if fictionUrl:
        try:
            resp = requests.get(fictionUrl, headers=HEADERS)
        except:
            print(f'{fictionUrl}\t连接失败，跳过')
            pass
        else:
            if checkStatus_code(resp.status_code):
                #Thread(target=saveResp2HTML, args=(resp, f'{title}.html')).start()  #保存获取的内容到文件
                index = fictionUrl.rfind('/')
                httpPrefix = fictionUrl[:index+1]
                # todo 解析最近更新的章节和总章节，两部分
                chapterParttern = r'.*?<dd><a href="(.*?)">(.*?)</a></dd>'
                recentParttern = r'class="listmain".*?</dt>(.*?)<dt>'
                recentList = re.findall(recentParttern, resp.content.decode('utf-8'), re.S)
                recentChapters =re.findall(chapterParttern, str(recentList), re.S)
                for i in range(len(recentChapters)):
                    recentChapters[i]=[httpPrefix+recentChapters[i][0],recentChapters[i][1]]
                allParttern = r'class="listmain".*?</dt>(.*?)</dl>'
                allList = re.findall(allParttern, resp.content.decode('utf-8'), re.S)
                allChapters = re.findall(chapterParttern, str(allList), re.S)
                for i in range(len(allChapters)):
                    allChapters[i]=[httpPrefix+allChapters[i][0],allChapters[i][1]]
                return {'recent': recentChapters, 'all': allChapters}
    return None


def getContentFromCapterUrl(chapterUrl):
    if chapterUrl:
        try:
            resp = requests.get(chapterUrl, headers=HEADERS)
        except:
            print(f'{chapterUrl}\t连接失败，跳过')
            pass
        else:
            if checkStatus_code(resp.status_code):
                contentPattern=r'.*?class="showtxt">(.*?)</div>'
                content = re.findall(contentPattern, resp.content.decode('utf-8'), re.S)[0]
                return content
    return None


def saveChapter2File(dir, chapterName, content):
    try:
        with open(os.path.join(dir, chapterName+'.txt'), 'w', encoding=('utf-8')) as f:
            f.write(content)
    except:
        print(f"保存章节《{chapterName}》失败")
        pass
    else:
        print(f"保存章节《{chapterName}》成功")

if __name__ == '__main__':
    if os.path.exists(TARGET_DIR)==False:
        TARGET_DIR = CURRENT_DIR
    for i in range(1):
        jianshuUrl = f'http://www.shuquge.com/category/7_{i}.html'
        fictions = parseArticleUrl(jianshuUrl)
        for fiction in fictions:
            if not os.path.exists(TARGET_DIR+'\\'+fiction[0]):
                os.mkdir(TARGET_DIR+'\\'+fiction[0])
            recentAndAll = getFictionDetailFromUrl(fiction[1], fiction[0])
            for chapter in recentAndAll['all']:
                chapterContent = getContentFromCapterUrl(chapter[0])        #获取到章节的内容
                t = Thread(target=saveChapter2File(TARGET_DIR+'\\'+fiction[0], chapter[1], chapterContent))
                t.start()
                t.join()


```







