# 20180919

### 作业

```
找一个需要登录的网站，使用re和xpath 方法爬取响应的数据，将数据保存为csv格式
```







###  示例代码

```python
# !/usr/bin/env python
# -*- coding:utf-8 -*-


import requests
import json

url ='https://fanyi.baidu.com/basetrans'        # url地址

headers = {'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'}

while True:
    words = input('您要查询的单词：')
    if words == 'X':
        break
    data = {'query': words,     # post请求需要请求的参数
            'from': 'zh',
            'to': 'en'}

    reponse = requests.post(url, headers=headers, data=data)
    reponse.encoding = 'utf-8'
    html = reponse.text

    text = json.loads(html)     # json --> python数据类型
    # print(text)
    # print(type(text))

    result = text['trans'][0]['dst']    # 通过dict进行按键键索引取得结果
    print('翻译的结果：', result)
    print('-' * 40)
```



```python
# !/usr/bin/env python
# -*- coding:utf-8 -*-

import requests
from lxml import etree
import csv


def get_html(url, headers, cookies):        # 获取页面内容
    req = requests.get(url, headers=headers, cookies=cookies)
    if req.status_code == 200:
        try:
            with open('dangdangbook.html', 'w', encoding='utf-8') as f:
                f.write(req.text)
        except:
            print('文件写入失败')
        else:           # 没有异常则执行此部分的内容
            # req.encoding = 'utf-8'
            return req.text
    else:
        return None


def get_content(htmls, num):     # 获得指定数据并保存到文件中
    elements = etree.HTML(htmls, parser=etree.HTMLParser(encoding='utf-8'))
    name = elements.xpath('//div[@class="name"]/a/text()')      # 书名
    link = elements.xpath('//div[@class="pic"]/a/@href')        # 书的链接
    pingNum = elements.xpath('//div[@class="star"]//a/text()')  # 评论数
    readtor = elements.xpath('//div[@class="publisher_info"]/a/@title') # 作者
    time = elements.xpath('//div[@class="publisher_info"]/span/text()') # 发布时间
    press = elements.xpath('//div[@class="publisher_info"]/a/text()')   # 出版社
    Ysprice = elements.xpath('//span[@class="price_n"]/text()')     # 原价
    Xsprice = elements.xpath('//span[@class="price_r"]/text()')     # 现价

    # with open('dangdangBook.txt', 'a', encoding='utf-8') as f:    # 将数据保存成txt格式
    #     for i in range(len(name)):
    #         f.write('%d'% (i+1))
    #         f.write(name[i]+'\t')
    #         f.write(link[i]+'\t')
    #         f.write(pingNum[i][:len(pingNum-3)]+'\t')
    #         f.write(readtor[i]+'\t')
    #         f.write(time[i]+'\t')
    #         f.write(press[i]+'\t')
    #         f.write(Ysprice[i]+'\t')
    #         f.write(Xsprice[i]+'\n')
    #         f.write('\n')
    # print('数据保存完成')


    with open('Top100.csv', 'a', encoding='utf_8_sig') as f:    # 将数据保存成csv格式
        spam = csv.writer(f, dialect='excel')       # 设置文件的打开方式，并将其转化为excel文件对象
        spam.writerow(['排名', '书名', '链接', '评论数', '作者', '时间', '出版社', '原价', '现价'])                                             # 设置文件的标题
        for i in range(len(name)):
            spam.writerow((str(i+1),        # 编号
                           str(name[i]),        # 书名
                           str(link[i]),
                           str(pingNum[i])[:len(pingNum)-3],    # 评论数
                           str(readtor[i]),     # 作者
                           str(time[i]),        # 发布时间
                           str(press[i]),       # 出版社
                           str(Ysprice[i]),     # 原件
                           str(Xsprice[i])))    # 现价
    print('写入成功！')


if __name__ == '__main__':
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
        'Referer': 'http://bang.dangdang.com/books/bestsellers/01.00.00.00.00.00-24hours-0-0-1-3',
        'Host': 'bang.dangdang.com',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'}

    strings = 'ddscreen=2; ddscreen=2; LOGIN_TIME=1537323849096; from=460-5-biaoti; order_follow_source=P-460-5-bi%7C%231%7C%23www.baidu.com%252Fs%253Fie%253Dutf-8%2526f%253D3%2526rsv_bp%253D1%2526rsv_idx%253D1%2526tn%253Dbaidu%2526wd%253D%2525E5%2525BD%252593%2525E5%2525BD%252593%2525E4%2525B9%2525A6%2525E5%25259F%25258E%2526oq%253D%7C%230-%7C-; ddscreen=2; __permanent_id=20180919091435236853160406264230310; __ddc_15d_f=1537319675%7C!%7C_utm_brand_id%3D11106; dest_area=country_id%3D9000%26province_id%3D111%26city_id%3D1%26district_id%3D1110101%26town_id%3D-1; NTKF_T2D_CLIENTID=guestF03A3BD8-A215-E629-DC7E-EF64EA8C6439; nTalk_CACHE_DATA={uid:dd_1000_ISME9754_guestF03A3BD8-A215-E6,tid:1537319692939616}; pos_6_end=1537319822152; pos_6_start=1537319826735; __ddc_1d=1537320537%7C!%7C_utm_brand_id%3D11106; __ddc_24h=1537320537%7C!%7C_utm_brand_id%3D11106; __ddc_15d=1537320537%7C!%7C_utm_brand_id%3D11106; __visit_id=20180919092856978356575854125810848; __out_refer=1537320537%7C!%7Cwww.baidu.com; producthistoryid=23597292%2C24198400; __dd_token_id=2018091910213911351257089580d0e9; permanent_key=2018091910213913420780575416cd67; login_dang_code=2018091910231847500133689181a28f; login.dangdang.com=.AYH=2018091910214009408670084&.ASPXAUTH=zIMlFQzQcBYsgbGJCCanV0K6D+Xlo5sU8QLeDyEXuYtUTUbU05UVLA==; dangdang.com=email=NGEyODAyOWNiM2U3NTliOEBkZG1vYmlsZV91c2VyLmNvbQ==&nickname=&display_id=1892506611567&customerid=8CkeltPFco4qLN1kUEtkhw==&viptype=Yk5qWrmjUho=&show_name=4a28029cb3e759b8; ddoy=email=4a28029cb3e759b8%40ddmobile_user.com&nickname=&agree_date=1&validatedflag=2&uname=4a28029cb3e759b8%40ddmobile_user.com&utype=0&.ALFG=off&.ALTM=1537323844; sessionID=pc_e3a3e3c054eeb4fc24438e064a5585bedc08a644ed360c5d3c90ff81e766a04c; __rpm=%7C...1537324387947; __trace_id=20180919103308554238599224122258128'
    # cookies中携带登录后，服务器发送的验证信息，保存到本地的浏览器


    cookies = {items.split('=')[0]:items.split('=')[1] for items in strings.split('; ')}
    # 通过字典推导式对cookies字符串进行处理

    start_num = int(input('请输入起始页：'))
    end_num = int(input('请输入终止页：'))

    for i in range(start_num, end_num+1):
        url = f'http://bang.dangdang.com/books/bestsellers/01.00.00.00.00.00-24hours-0-0-1-{i}'     # 构造url地址
        html = get_html(url, headers, cookies)   # 获得网页内容
        get_content(html, i)    # 获得处理的数据并保存
```





### 笔记部分

>1、爬虫的流程：
>    1、url
>    2、模拟浏览器向服务发送请求，获得响应
>        import requests
>
>    headers = {'User-Agent':''}
>    req = requests.get(url, headers=headers, cookies=cookies)
>    reponse = requests.post(url, headers=headers, data=data)
>
>    获得响应的内容：
>        1、req.text
>        2、req.content()
>
>    如果有编码：
>        1、req.encoding = 'utf-8'
>        2、req.content().decode()
>
>    如果结果是json格式：
>        import json
>
>        json.loads('strings')    # json格式转化为Python的数据类型
>        json.dumps('strings')    # Python的数据类型转化为json格式
>
>3、解析处理出想要的数据
>    1、import re
>        re.findall(pattern, htmls, re.S)
>
>    2、xpath
>        from lxml import etree
>
>        elements = etree.HTML(htmls)    # 将获取到的页面内容转化为elements对象
>        elements.xpath('')              # 使用xpath方法选择节点获得相关的数据
>
>        符号：
>            1、‘/’ :选择节点
>                ‘html/body/div/’   # html -> body -> div
>
>            2、'@':获取标签下的属性值；修饰标签中的属性（定位）
>                ‘div/a/@href’       # div -> a的href属性的值
>                ‘//div[@class="name"]’      # 查找div中class="name"的内容
>
>            3、'//' : 从任意节点开始选择
>                ‘//div’     # 从任意位置的div标签开始
>
>            4、获取标签中的值
>                ‘//p[@class="price_e"]/text()’      # 取class="price_e"的p标签下文本
>                ‘//p[@class="price_e"]//text()’     # 取class="price_e"的p标签下多行文本
>
>4、保存数据
>    1、将数据保存为txt格式：
>        with open(files.txt, mode, encoding='utf-8') as f:
>            f.write(strings)
>
>    2、将数据保存为csv格式：
>        import csv
>
>        with open('files.csv', mode, encoding='utf_8_sig') as f:    # 将数据保存成csv格式
>            spam = csv.writer(f, dialect='excel')       # 设置文件的打开方式，并将其转化为excel文件对象
>            spam.writerow(['排名', '书名', '链接', '评论数', '作者', '时间', '出版社', '原价', '现价'])     # 标题                                        # 设置文件的标题
>            for i in range(len(name)):
>                spam.writerow((str(i+1))    # 写入数据
>
>



